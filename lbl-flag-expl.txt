cpu_time ==> CPU time ==> -l s_cpu= ==>
This is the amount of CPU time your job needs to complete:<br> 
&bull; you can enter a time (any time limit) or <br>
&bull; select a queue length (form the pull down menu).<p>
The format is <tt>MM</tt>, <tt>MMM</tt>, <tt>H:MM</tt>, <tt>HH:MM</tt>,
<tt>D:HH:MM</tt> or <tt>DD:HH:MM</tt>. <br>
This is <em>not</em> the format used by <tt>qsub</tt> which is either S or
H:M:S (so s_cpu=H:M:S) 
<p>
Hence the minimum is 10 minutes, while the maximum is set to 30:00:00 
(30 days, corresponding to the <tt>lTh?.q</tt> queues limit). <p>
A value of "-" will direct the job to the unlimited queues: no time limit, but
fewer slots.
---
memory ==> Memory ==> -l mres= ==>
This is the amount of memory <b>per CPU</b> your job will use (hence need).<p>

The total amount of reserved memory will be  this amount <b>multiplied</b> by 
the number of CPUs allocated.<p>
<blockquote>
The cluster is a shared resource, and memory is a limited and costly resource, 
so do not set this value to more than you will use.
Doing so will prevent access to that memory, hence other jobs from running!</blockquote>
---
pe ==> ==> -pe ==>
PE: parallel environment:<br>
 &bull; serial: 1 CPU/job; <br>
 &bull; MPI: message passing interface, CPUs can be on different nodes: <br>
 &nbsp; &nbsp; <tt>orte</tt> and <tt>mpich</tt> are two flavors of MPI; <br>
 &bull; multi-thread: shared memory model, all CPUs  must be on the same node
 (aka <tt>openmp</tt>). <p>
<b>NOTE</b>: for high memory jobs the PE can only be "serial" or "multi-thread".
---
nbr_cpu ==> Number of CPUs ==> -pe ==>
This is the number of CPUs/threads or "slots" your job will use: <br>
 &bull; MPI jobs can use a large number of CPUs, as they are distributed over nodes; <br>
 &bull; Multi-threaded jobs are limited to 64 or 40 threads, since all the threads
    need to be on the same node. <p>
The allocated number of slots will stored in the variable NSLOTS.
<p>
The compute nodes in the hi-cpu queues have up to 64 cores, the compute nodes
in the hi-memory queues have 24, 64 or 80 cores (two, twelve, and two nodes
respectively; with 512GB, 512GB and 1TB of total memory respectively).
---
shell ==> ==>  ==> 
The choice of shell determines the syntax you use for your commands. <br>
Please use sh and not bash, as bash has some peculiarities that could lead to
unexpected/strange behavior.
---
other ==> ==> ==> 
These are optional parameters.
---
job_name ==> Job Name ==> -N ==>
This is the name of your job, it can be any valid string.
---
log_name ==> Log File Name ==> -o ==>
This is the filename where the output of your script will go 
make sure to use a unique filename for each concurrent job.<p>

If the file exists, the output will be appended to the existing file.
---
err_name ==> Err File Name ==> -e ==> 
This is the filename where the error messages of your script will go 
make sure to use a unique filename for each concurrent job.<p>

If the file exists, the output will be appended to the existing file.
---
email_add ==> Email ==> -M ==>
The email address to send job status notifications.
---
add_modules ==> Select which modules to add: ==> ==>
Select from the list which modules your job will need.
---
job_commands ==> Job specific commands: ==> ==>
Enter here the commands for your job, you must follow the syntax of the
selected shell.
---
commands     ==> Type your commands here      ==>        ==> ---
goto_cwd     ==> Change to cwd                ==> -cwd   ==> ---
join_err     ==> Join stderr &amp; stdout     ==> -j y   ==> ---
send_email   ==> Send email notifications     ==> -m abe ==> ---
modules      ==> from this list               ==>        ==> 
